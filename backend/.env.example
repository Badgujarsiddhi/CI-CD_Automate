# Ollama Configuration
# For local development, use:
OLLAMA_BASE_URL=http://localhost:11434

# For production (when Ollama is hosted separately), use:
# OLLAMA_BASE_URL=https://your-ollama-service.railway.app

# Model to use (recommended for 8GB RAM: llama3.2:3b)
OLLAMA_MODEL=llama3.2:3b

# Alternative models you can use:
# OLLAMA_MODEL=mistral:7b      # Better quality, requires more RAM (~4GB)
# OLLAMA_MODEL=phi3:mini       # Good for medical tasks (~2GB)
