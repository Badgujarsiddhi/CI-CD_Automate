# Ollama Configuration
# For local development, use:
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (recommended for 8GB RAM: llama3.2:3b)
OLLAMA_MODEL=llama3.2:3b
